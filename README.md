# Neural Network Architecture Explorer

A comprehensive educational project for experimenting with neural network depth, width, and hyperparameters using both PyTorch and TensorFlow. Perfect for understanding how different architectural choices impact model performance on tabular data.

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.13+-orange.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

##  Overview

This project uses the **Wine Quality Dataset** (physicochemical properties â†’ quality prediction) to systematically explore:

- **Network Depth**: Impact of adding more layers
- **Network Width**: Impact of more neurons per layer
- **Activation Functions**: ReLU, Tanh, Sigmoid, ELU/LeakyReLU
- **Learning Rate**: Effect on convergence and performance
- **Regularization**: Dropout and L2 regularization
- **Framework Comparison**: PyTorch vs TensorFlow implementations

## Project Structure

```
neural-networks-study/
â”‚
â”œâ”€â”€ experiment_pytorch.py          # PyTorch experiments
â”œâ”€â”€ experiment_tensorflow.py       # TensorFlow experiments
â”œâ”€â”€ download_data.py               # Dataset downloader
â”œâ”€â”€ setup_project.py              # Project structure setup
â”œâ”€â”€ app.py                        # Streamlit dashboard
â”œâ”€â”€ api.py                        # FastAPI backend
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md                     # This file
â”‚
â”œâ”€â”€ data/                         # Dataset directory
â”‚   â””â”€â”€ winequality-red.csv
â”‚
â”œâ”€â”€ results/                      # Experiment results
â”‚   â”œâ”€â”€ pytorch_summary.csv
â”‚   â”œâ”€â”€ tensorflow_summary.csv
â”‚   â””â”€â”€ *.json                    # Detailed results
â”‚
â”œâ”€â”€ models/                       # Saved models (optional)
â””â”€â”€ notebooks/                    # Jupyter notebooks (optional)
```

### Step 1: Clone the Repository

```bash
git clone https://github.com/RamuNalla/neural-networks-study.git
cd neural-networks-study
```

### Step 2: Create Virtual Environment

```bash
python -m venv venv
source venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Setup Project Structure

```bash
python setup_project.py
```

### Step 5: Download Dataset

```bash
python download_data.py
```

## ðŸš€ Quick Start

### Run All Experiments

```bash
python experiment_pytorch.py        # PyTorch experiments (takes ~10-15 minutes)

python experiment_tensorflow.py     # TensorFlow experiments (takes ~10-15 minutes)
```

### Launch Visualization Dashboard

```bash
streamlit run app.py
```

Open your browser to `http://localhost:8501`

### Start API Server

```bash

python -m uvicorn api:app --reload  # Terminal 1: Start API

curl http://localhost:8000/summary  # Terminal 2: Test API
```

## ðŸ”¬ Experiments

### Experiment Categories

#### 1. **Depth Experiments** (Fixed Width = 64)
- `shallow_network`: 1 hidden layer [64]
- `medium_depth`: 2 hidden layers [64, 64]
- `deep_network`: 4 hidden layers [64, 64, 64, 64]
- `very_deep`: 6 hidden layers [64, 64, 64, 64, 64, 64]

**Learn**: How depth affects learning capacity and training time

#### 2. **Width Experiments** (Fixed Depth = 2)
- `narrow_network`: [16, 16]
- `medium_width`: [64, 64]
- `wide_network`: [256, 256]
- `very_wide`: [512, 512]

**Learn**: How width affects model capacity and overfitting

#### 3. **Activation Function Experiments**
- `activation_relu`: ReLU (baseline)
- `activation_tanh`: Tanh
- `activation_sigmoid`: Sigmoid
- `activation_leaky_relu` / `activation_elu`: Advanced activations

**Learn**: Impact of non-linearity choices

#### 4. **Learning Rate Experiments**
- `lr_low`: 0.0001
- `lr_high`: 0.01
- Baseline: 0.001

**Learn**: Convergence speed vs stability trade-off

#### 5. **Regularization Experiments**
- `with_dropout`: Dropout rate = 0.3
- `with_l2`: L2 regularization = 0.01

**Learn**: Preventing overfitting techniques